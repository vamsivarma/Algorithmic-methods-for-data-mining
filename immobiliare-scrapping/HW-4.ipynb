{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Does basic house information reflect house's description?\n",
    "## Group 11 - \n",
    "### The homework consists of a clustering analysis of house announcements in Rome from Immobiliare.it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile load_packages.py\n",
    "\n",
    "import time\n",
    "\n",
    "# For webscrapping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from os.path import join as pjoin\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import html\n",
    "\n",
    "from lxml import html\n",
    "\n",
    "# For persisting indexes in an external file\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "\n",
    "# For word tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# For stop words list\n",
    "from nltk.corpus import stopwords\n",
    "# For word stemming\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load utility functions to be used through out the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile utilities.py\n",
    "\n",
    "# Common utility functions\n",
    "\n",
    "def extract_number(n):\n",
    "    n = n.replace('.', '')\n",
    "    n = n.replace('+', '')\n",
    "    n_list = [str(s) for s in n.split() if s.isdigit()]\n",
    "\n",
    "    s = ''.join(n_list).strip()\n",
    "    \n",
    "    if not s:\n",
    "        s = 0\n",
    "\n",
    "    return int(s)\n",
    "\n",
    "# Utility functions for reading and writing files using pickel python package\n",
    "def read_file_from_pickle(file):\n",
    "    file_content = {}\n",
    "    \n",
    "    if file.is_file():\n",
    "        with open(file, \"rb\") as f:\n",
    "            file_content = pickle.load(f)\n",
    "            f.close()\n",
    "    \n",
    "    return file_content\n",
    "\n",
    "def write_file_to_pickle(file, content):\n",
    "    with open(file, \"wb\") as f:\n",
    "        pickle.dump(content, f)\n",
    "        f.close()\n",
    "\n",
    "# Apply Jaccard similarity to find out 3 most similar clusters\n",
    "def get_jaccard(a, b):\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "\n",
    "# Utility functions for clustering and wordcloud\n",
    "\n",
    "def get_listing_content(i, sflag):\n",
    "    listing_words = ''\n",
    "    if sflag:\n",
    "        listing_words = listing_content_persist[i]\n",
    "    else:\n",
    "        listing_id = listing_index_persist['listing_ids'][i]\n",
    "        listing_data = listings_persist[listing_id]\n",
    "        listing_words = listing_data['description']\n",
    "\n",
    "    return listing_words + ' '\n",
    "\n",
    "def get_wc_save_path(i, sflag):\n",
    "\n",
    "    f_name_prefix = ''\n",
    "\n",
    "    if(sflag):\n",
    "        f_name_prefix = \"wordcloud\"\n",
    "    else:\n",
    "        f_name_prefix = \"wordcloud_all\"\n",
    "\n",
    "    return f_name_prefix + \"/cluster_\" + str(i)\n",
    "\n",
    "\n",
    "def compare_clusters(c1, c2):\n",
    "\n",
    "    jac_score_list = []\n",
    "    comb_list = []\n",
    "    cmp_output = {}\n",
    "\n",
    "    for i in range(len(c1)):\n",
    "        for j in range(len(c2)):\n",
    "            # Adding the score of each cluster combination to jac_score_list\n",
    "            jac_score_list.append(get_jaccard(set(c1[i]), set(c2[j])))\n",
    "            comb_list.append([i,j])\n",
    "\n",
    "    cmp_output['score_list'] = jac_score_list\n",
    "    cmp_output['comb_list'] = comb_list\n",
    "    \n",
    "    return cmp_output\n",
    "\n",
    "\n",
    "def get_similar_clusters(top_3_list, c1, c2):\n",
    "    similar_clusters = []\n",
    "\n",
    "    for t in top_3_list:\n",
    "        t_list = list(t)\n",
    "        c_list_1 = c1[t_list[1][0]]\n",
    "        c_list_2 = c2[t_list[1][1]]\n",
    "\n",
    "        s_list = list(set(c_list_1 + c_list_2))\n",
    "\n",
    "        similar_clusters.append(s_list)\n",
    "\n",
    "    return similar_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data persisted using pickel library:\n",
    "\n",
    "### We are using the following indexes - \n",
    "\n",
    "#### 1. listings.pkl - contains all the listings data as it was scrapped in the dictionary format of \"listing_id\": listing_obj\n",
    "\n",
    "#### 2. listing_links.pkl - contains all the individual listing page hyperlinks to be scrapped separately (one time action)\n",
    "\n",
    "#### 3. listing_index.pkl - contains all the listing ids(extracted from the listing pages while scrapping), to preserve the ordering of listings while building information and description datasets\n",
    "\n",
    "#### 4. listing_content.pkl - contains the data from all individual listings with all the stop words removed, we are using this to find tfidf values of all the words in the vocabulary.pkl file\n",
    "\n",
    "#### 5. vocabulary.pkl - contains metadata of the words in the following format (eg: \"123\": [334,4545,645]) where 123 is the word_id taken from words.pkl and 334,4545,645 are listing ids in which that word is present\n",
    "\n",
    "#### 6.  words.pkl - contains all the words present accross all the listings in the following format (eg: \"house\": \"123\")\n",
    "\n",
    "#### 7. iindex_tf_idf.pkl - contains the dictionary of tfidf values of all the words in words.pkl\n",
    "\n",
    "#### 8. information_dataset.pkl - Holds the information data set in the format mentioned in the homework text\n",
    "\n",
    "#### 9. description_dataset.pkl -  Holds the description data set in the format mentioned in the homework text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile load_data.py\n",
    "\n",
    "# Import all the persisted data at once\n",
    "# Data to import: \n",
    "# Listing data, Individual listing links, Listing Index \n",
    "# Words, Vocabulary, listing_content, iindex_tf_idf\n",
    "# information_dataset, description_dataset\n",
    "\n",
    "# Path to the current working directory to refer to all the files relatively\n",
    "my_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "# Datastructures for holding the listings and other metadata\n",
    "# Please create a directory(in your current working directory) with name 'indexes'  \n",
    "\n",
    "#Holds individual listings data for the listing pages downloaded\n",
    "listings_file = Path(os.path.join(my_path, \"indexes/listings.pkl\"))\n",
    "listings_persist = {}\n",
    "\n",
    "if listings_file.is_file():\n",
    "    with open(listings_file, \"rb\") as listings:\n",
    "        listings_persist = pickle.load(listings)\n",
    "        listings.close()\n",
    "\n",
    "#Holds the URLs of individual listings for extracting complete description of a particular listing\n",
    "listing_links_file = Path(os.path.join(my_path, \"indexes/listing_links.pkl\"))\n",
    "listing_links_persist = read_file_from_pickle(listing_links_file)\n",
    "\n",
    "\n",
    "#Holds the order of individual listing\n",
    "listing_index_file = Path(os.path.join(my_path, \"indexes/listing_index.pkl\"))\n",
    "listing_index_persist = read_file_from_pickle(listing_index_file)\n",
    "     \n",
    "\n",
    "# Retrieving persisted information for listing content and word map (words and vocabulary)\n",
    "content_file = Path(os.path.join(my_path, \"indexes/listing_content.pkl\"))\n",
    "listing_content_persist = read_file_from_pickle(content_file)\n",
    "\n",
    "\n",
    "\n",
    "vocabulary_file = Path(os.path.join(my_path, \"indexes/vocabulary.pkl\"))\n",
    "vocabulary_persist = read_file_from_pickle(vocabulary_file)\n",
    "\n",
    "words_file = Path(os.path.join(my_path, \"indexes/words.pkl\"))\n",
    "words_persist = read_file_from_pickle(words_file)\n",
    "        \n",
    "index_file = Path(os.path.join(my_path, \"indexes/iindex_tf_idf.pkl\"))\n",
    "iindex_tf_idf_persist = read_file_from_pickle(index_file)\n",
    "\n",
    "\n",
    "# Information data set\n",
    "information_ds_file = Path(os.path.join(my_path, \"indexes/information_dataset.pkl\"))\n",
    "information_ds_persist = read_file_from_pickle(information_ds_file)\n",
    "\n",
    "# Description data set - containing tf-idf values\n",
    "description_ds_file = Path(os.path.join(my_path, \"indexes/description_dataset.pkl\"))\n",
    "description_ds_persist = read_file_from_pickle(description_ds_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download group listing pages:  \n",
    "#### Note: We downloaded the files in to folder with name data. Every group listing page has information about 25 listings, we downloaded 1000 pages to have a bigger sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group listings pages already downloaded\n"
     ]
    }
   ],
   "source": [
    "#%%writefile group_listings_download.py\n",
    "\n",
    "# Please create a directory 'data' \n",
    "# Checking if the pages are already downloaded\n",
    "# Check the first file \n",
    "listing_page_file = Path(os.path.join(my_path, \"data/listing_0.html\"))\n",
    "\n",
    "if listing_page_file.is_file() == False:\n",
    "\n",
    "    # If there are no files then start downloading each html file with a delay of 3 seconds\n",
    "    print('Downloading group listings pages...')\n",
    "    \n",
    "    url_root = 'https://www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag='\n",
    "\n",
    "\n",
    "    for i in range(1000):\n",
    "\n",
    "        cur_url = url_root + str(i)\n",
    "\n",
    "        cur_content = requests.get(cur_url)\n",
    "\n",
    "        res_text = BeautifulSoup(cur_content.text, \"lxml\")\n",
    "\n",
    "        cur_html_file= open(\"data/listing_\" + str(i) + \".html\", \"w\")\n",
    "        cur_html_file.write(str(res_text))\n",
    "        cur_html_file.close()\n",
    "        \n",
    "        # 3 seconds delay for the next page download attempt\n",
    "        time.sleep(3)\n",
    "else:\n",
    "    print('Group listings pages already downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download individual listing page to update the full description of the listing, we downloaded 10000 individual listing pages and updated corresponding index file listings.pkl\n",
    "\n",
    "####  Note: We downloaded the files in to folder with name listing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual listing pages already downloaded\n"
     ]
    }
   ],
   "source": [
    "#%%writefile listings_download.py\n",
    "\n",
    "\n",
    "# Downloading individual listings pages from the listing links persisted\n",
    "# Please create a directory data_detail\n",
    "# Checking if the pages are already downloaded\n",
    "# Check the first file \n",
    "listing_detail_file = Path(os.path.join(my_path, \"data_detail/listing_detail_0.html\"))\n",
    "\n",
    "\n",
    "if listing_detail_file.is_file() == False:\n",
    "    \n",
    "    print('Downloading individual listing pages...')\n",
    "\n",
    "    links_list = []\n",
    "\n",
    "    if(len(listing_links_persist.keys()) != 0):\n",
    "\n",
    "        # Getting listing page links\n",
    "        for key in  listing_links_persist:\n",
    "            cur_link = listing_links_persist[key]\n",
    "\n",
    "            # Check if the link is relative\n",
    "            # If yes make it absolute link\n",
    "            # Need to add better checks here\n",
    "            if(cur_link[0] == \"/\"):\n",
    "                cur_link = \"https://www.immobiliare.it\" + cur_link\n",
    "\n",
    "            links_list.append(cur_link)\n",
    "\n",
    "        # Downloading the pages\n",
    "        for i in range(690, len(links_list)):\n",
    "\n",
    "            cur_url = links_list[i]\n",
    "\n",
    "            cur_content = requests.get(cur_url)\n",
    "\n",
    "            res_text = BeautifulSoup(cur_content.text, \"lxml\")\n",
    "\n",
    "            cur_detail_file = os.path.join(my_path, \"data_detail/listing_detail_\" + str(i) + \".html\")\n",
    "\n",
    "            cur_html_file= open(cur_detail_file, \"w\")\n",
    "            cur_html_file.write(str(res_text))\n",
    "            cur_html_file.close()\n",
    "\n",
    "            # Wait for 3 seconds before downloading the next page\n",
    "            time.sleep(3)\n",
    "else:\n",
    "    print('Individual listing pages already downloaded')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do web scrapping (Using BeautifulSoup) for group listing and individual listing pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes are already created\n"
     ]
    }
   ],
   "source": [
    "#%%writefile scrape_listings.py\n",
    "\n",
    "# If this is True then the listing description is updated from \n",
    "# the individual pages\n",
    "desc_flag = False\n",
    "\n",
    "if(len(listing_index_persist.keys()) == 0):\n",
    "\n",
    "    print(\"Indexes are being created\")\n",
    "\n",
    "    l_index = 0\n",
    "    listing_index_persist['listing_ids'] = []\n",
    "\n",
    "    # Every page has 25 listings so\n",
    "    # 410*25 will be more than 10000 listings\n",
    "    for i in range(1, 410):\n",
    "\n",
    "        cur_listing_page = BeautifulSoup(open(os.path.join(my_path, 'data/listing_' + str(i) + '.html')), 'html.parser')\n",
    "\n",
    "        listing_container = cur_listing_page.find(class_=\"annunci-list\")\n",
    "        \n",
    "        # Need to improve exception handling in this loop\n",
    "        for cur_listing in listing_container.find_all(class_=[\"listing-item\", \"js-row-detail\"], recursive=False):\n",
    "\n",
    "            listing_dict = {\n",
    "                \"id\": \"\",\n",
    "                \"listing_id\": \"\",\n",
    "                \"title\": \"\",\n",
    "                \"price\": 0,\n",
    "                \"locali\": 0,\n",
    "                \"superficie\": 0,\n",
    "                \"bagni\": 0,\n",
    "                \"piano\": 0,\n",
    "                \"immobile\": \"\",\n",
    "                \"listing_link\": \"\",\n",
    "                \"description\": \"\"\n",
    "            }\n",
    "\n",
    "            listing_body = cur_listing.find(class_=\"listing-item_body\")\n",
    "\n",
    "            if(listing_body):\n",
    "\n",
    "                listing_dict['id'] = l_index\n",
    "                listing_dict['listing_id'] = cur_listing.get(\"data-id\")\n",
    "\n",
    "                listing_dict['title'] = listing_body.find(class_=\"titolo\").text.strip()\n",
    "\n",
    "                listing_dict[\"listing_link\"] = listing_body.find(\"a\", {\"id\": \"link_ad_\" + listing_dict['listing_id']}).get(\"href\")\n",
    "\n",
    "                listing_dict['description'] = listing_body.find(class_=\"descrizione\").text.strip()\n",
    "\n",
    "                # Extracting the listing features \n",
    "                listing_features = listing_body.find(class_=[\"listing-features\", \"list-piped\"])\n",
    "\n",
    "                listing_links_persist[listing_dict['listing_id']] = listing_dict[\"listing_link\"]\n",
    "\n",
    "                for cur_feature in listing_features.find_all(class_=\"lif__item\", recursive=False):\n",
    "\n",
    "                    feature_cls_list = cur_feature.get(\"class\")\n",
    "\n",
    "                    # Extract listing price\n",
    "                    if 'lif__pricing' in feature_cls_list:\n",
    "                        listing_dict['price'] = extract_number(cur_feature.text.strip())\n",
    "                    else:\n",
    "                        # Extract other features information\n",
    "                        # @TODO: Need to refine locali to contain a list: example: 1-5 should be [1,2,3,4,5]\n",
    "                        feature_name = cur_feature.find(class_=\"lif--muted\")\n",
    "\n",
    "                        # @TODO: Need to do this more efficiently\n",
    "                        if(feature_name):\n",
    "                            feature_name = feature_name.text.strip()\n",
    "\n",
    "                            if feature_name in listing_dict:\n",
    "                                feature_value = cur_feature.find(class_=\"text-bold\").text.strip()\n",
    "                                listing_dict[feature_name] = extract_number(feature_value)\n",
    "\n",
    "\n",
    "                listing_index_persist['listing_ids'].append(listing_dict['listing_id'])\n",
    "\n",
    "                l_index += 1\n",
    "                listings_persist[listing_dict['listing_id']] = listing_dict\n",
    "\n",
    "\n",
    "    # Remove duplicate listing entries\n",
    "    listing_index_persist['listing_ids'] = list(set(listing_index_persist['listing_ids']))\n",
    "\n",
    "    # Persist the listings object and dictionary using pickel library\n",
    "    \n",
    "    #Save listings data\n",
    "    write_file_to_pickle(listings_file, listings_persist)\n",
    "\n",
    "    #Save individual listings links data\n",
    "    write_file_to_pickle(listing_links_file, listing_links_persist)\n",
    "\n",
    "    #Save index of listings\n",
    "    write_file_to_pickle(listing_index_file, listing_index_persist)\n",
    "\n",
    "else:\n",
    "    print(\"Indexes are already created\")\n",
    "\n",
    "\n",
    "#print(\"No of links:\")\n",
    "#print(len(listing_links_persist.keys()))\n",
    "\n",
    "#print(\"No of listings:\")\n",
    "#print(len(listings_persist.keys()))\n",
    "\n",
    "#print(\"No of listings in the listing index file:\")\n",
    "#print(len(listing_index_persist['listing_ids']))\n",
    "\n",
    "\n",
    "if desc_flag:\n",
    "\n",
    "    # Parse the detail pages \n",
    "    # And update the description of individual listings\n",
    "    for i in range(len(listing_links_persist.keys())):\n",
    "        cur_detail_page = BeautifulSoup(open(os.path.join(my_path, 'data_detail/listing_detail_' + str(i) + '.html')), 'html.parser')\n",
    "\n",
    "        cur_page_contact = cur_detail_page.find('div',{\"id\":\"up-contact-box\"})\n",
    "        if cur_page_contact:\n",
    "            cur_page_elem = cur_page_contact.find(class_=\"info-agenzia\")\n",
    "\n",
    "            if cur_page_elem:\n",
    "                cur_page_id = cur_page_elem.get(\"data-annuncio\")\n",
    "\n",
    "                cur_page_description =  cur_detail_page.find(class_=\"description-text\")\n",
    "\n",
    "                if cur_page_description:\n",
    "                    cur_page_description = cur_page_description.text.strip()\n",
    "\n",
    "                    cur_page_description = \"\".join(cur_page_description.splitlines())\n",
    "\n",
    "                    if cur_page_id in listings_persist:\n",
    "                        listings_persist[cur_page_id]['description'] = cur_page_description\n",
    "                    else:\n",
    "                        pass\n",
    "                        #print(\"Page key not found in the persisted data\")\n",
    "                else:\n",
    "                    pass\n",
    "                    #print(\"Page Description not found\")\n",
    "            else:\n",
    "                pass\n",
    "                #print(\"Page ID not found\")\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"Contact not found\")\n",
    "\n",
    "    #Save listings data with new content (complete listing description)\n",
    "    write_file_to_pickle(listings_file, listings_persist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information data set already present\n",
      "9987\n"
     ]
    }
   ],
   "source": [
    "#%%writefile create_information_ds.py\n",
    "\n",
    "#Preparing information data set\n",
    "if(len(information_ds_persist.keys()) == 0):\n",
    "\n",
    "    information_ds_persist['dataset'] = []\n",
    "\n",
    "    # Get the persisted listings data\n",
    "    for listing_id in listing_index_persist['listing_ids']:\n",
    "        cur_listing = listings_persist[listing_id]\n",
    "\n",
    "        listing_info = [cur_listing['price'], cur_listing['locali'], cur_listing['superficie'], cur_listing['bagni'], cur_listing['piano']]\n",
    "\n",
    "        information_ds_persist['dataset'].append(listing_info)\n",
    "    \n",
    "    #Save information data set\n",
    "    write_file_to_pickle(information_ds_file, information_ds_persist)\n",
    "\n",
    "else:\n",
    "    print(\"Information data set already present\")\n",
    "\n",
    "\n",
    "print(len(information_ds_persist['dataset']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vamsigunturi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Vocabulary data set already present\n"
     ]
    }
   ],
   "source": [
    "#%%writefile create_vocabulary.py\n",
    "\n",
    "#First we import stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "#To remove punctuation we use regexptokenizer, but we leave dollar symbol $ because maybe is used in some queries\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$')\n",
    "#we create the stemmer\n",
    "ps = SnowballStemmer('italian')\n",
    "\n",
    "list_len = len(listing_index_persist['listing_ids'])\n",
    "\n",
    "if(len(listing_content_persist.keys()) == 0):\n",
    "    \n",
    "    listing_word_map = {}\n",
    "    \n",
    "    # We reach here if we don't have indexes already present\n",
    "    print(\"Vocabulary is being created...\")\n",
    " \n",
    "    for i in range(list_len):\n",
    "        \n",
    "        cur_list_id = listing_index_persist['listing_ids'][i]\n",
    "        \n",
    "        cur_list_obj = listings_persist[cur_list_id]\n",
    "\n",
    "        # Extract all the text in the individual listing\n",
    "        # For listing title\n",
    "        t1 = cur_list_obj['title']\n",
    "        \n",
    "        # For listing content\n",
    "        t2 = cur_list_obj['description']\n",
    "        \n",
    "        t = t1+ ' ' +t2\n",
    "        t = t.lower()\n",
    "        t = tokenizer.tokenize(t)\n",
    "        \n",
    "        # This array will contain all the valid words in a given review after removing \n",
    "        # all the stop words, punctuations, stemming etc..,, we will use this information\n",
    "        # to find out the term frequency there by tf-idf values\n",
    "        listing_words = []\n",
    "        \n",
    "        for r in t :\n",
    "            if not r in stop_words:\n",
    "                sr = r #ps.stem(r) - avoid stemming for now for the wordcloud\n",
    "                \n",
    "                listing_words.append(sr)\n",
    "                \n",
    "                if not  sr in listing_word_map:\n",
    "                    listing_word_map[sr] = [i]\n",
    "                else:\n",
    "                    listing_word_map[sr]+=[i]\n",
    "                    \n",
    "                    \n",
    "        listing_content_persist[i] = ' '.join(listing_words)\n",
    "    \n",
    "    # Saving the content and indexes for the first time\n",
    "    # We made use of pickel python module\n",
    "    #Saving content dictionary\n",
    "    write_file_to_pickle(content_file, listing_content_persist)\n",
    "    \n",
    "    # Word and Vocabulary indexes based on word map\n",
    "    c = 0\n",
    "    for key in listing_word_map:\n",
    "        words_persist[key] = c\n",
    "        vocabulary_persist[c] = listing_word_map[key]\n",
    "        c += 1\n",
    "    \n",
    "    #Save vocabulary and words\n",
    "    write_file_to_pickle(vocabulary_file, vocabulary_persist)\n",
    "    write_file_to_pickle(words_file, words_persist)\n",
    "else:\n",
    "    print(\"Vocabulary data set already present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tfidfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Indexes already present\n"
     ]
    }
   ],
   "source": [
    "#%%writefile calculate_tfidfs.py\n",
    "\n",
    "if(len(iindex_tf_idf_persist.keys()) == 0):\n",
    "    \n",
    "    print(\"Inverted Indexes are being calculated\")\n",
    "\n",
    "    word_iindex = {}\n",
    "\n",
    "    #Creating inverted index using tf-idf and consine similarity\n",
    "    for word in words_persist:\n",
    "        word_doc_list = vocabulary_persist[words_persist[word]]\n",
    "        word_iindex[word] = []\n",
    "\n",
    "        # Store indexes based on number of times a particular word is present in a given document\n",
    "        for doc in word_doc_list:\n",
    "            doc_content = listing_content_persist[doc]\n",
    "            # Pushing the term frequency with document id\n",
    "            word_iindex[word].append([doc, doc_content.split().count(word)])\n",
    "\n",
    "    # Store indexes based on tf-idf\n",
    "    docs_length = len(listing_content_persist.keys())\n",
    "    iindex_tf_idf_persist = word_iindex\n",
    "\n",
    "    for key, word in iindex_tf_idf_persist.items():\n",
    "        # find out the relative importance of a particular terms relating it to document count\n",
    "        idf= math.log10( docs_length / len(word) )\n",
    "\n",
    "        for elem in word:\n",
    "            # Add the document score corresponding to a particular term which we then use in the \n",
    "            # search results ranking of documents\n",
    "            elem[1] = idf * elem[1]\n",
    "    \n",
    "\n",
    "    # Persisting the indexes calculated \n",
    "    write_file_to_pickle(index_file, iindex_tf_idf_persist)\n",
    "else:\n",
    "    print(\"Inverted Indexes already present\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create description dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description data set already present\n"
     ]
    }
   ],
   "source": [
    "#%%writefile create_description_ds.py\n",
    "\n",
    "# Preparing Description data set\n",
    "# Create description data set\n",
    "# Extract the words in individual listings\n",
    "# Create a matrix with rows as listings and columns as words\n",
    "# Combile listing title and description\n",
    "# Remove stop words\n",
    "# Calculate the term frequency\n",
    "# Calculate the td*idf score for that word in that document\n",
    "# Which gives the description data set for the 10000 listings saved\n",
    "if(len(description_ds_persist.keys()) == 0):\n",
    "\n",
    "    print(\"Description data set is being created...\")\n",
    "\n",
    "    list_len = len(listing_index_persist['listing_ids'])\n",
    "    description_ds = []\n",
    "    \n",
    "    #Build the description data set\n",
    "    for i in range(list_len):\n",
    "        \n",
    "        cur_list_id = listing_index_persist['listing_ids'][i]\n",
    "        \n",
    "        cur_list_obj = listings_persist[cur_list_id]\n",
    "\n",
    "        cur_word_list = []\n",
    "        \n",
    "        #Initialize each word tf-idf with 0's\n",
    "        for word in words_persist:\n",
    "            cur_word_list.append(0)\n",
    "\n",
    "        # @TODO: Need to optimize the number of verfications done here\n",
    "        for key, word in iindex_tf_idf_persist.items():\n",
    "            # elem[0] - list_id\n",
    "            # elem[1] - tf-idf\n",
    "            for elem in word:\n",
    "                # Update tf-idf of that word for that listing \n",
    "                if(elem[0] == i):\n",
    "                    cur_word_list[words_persist[key]] = elem[1]\n",
    "        \n",
    "        description_ds.append(cur_word_list)\n",
    "\n",
    "    description_ds_persist['dataset'] = description_ds\n",
    "\n",
    "\n",
    "    # Persisting the indexes calculated \n",
    "    write_file_to_pickle(description_ds_file, description_ds_persist)\n",
    "    \n",
    "else:\n",
    "    print(\"Description data set already present\")\n",
    "    \n",
    "#print(description_ds_persist['dataset'][789])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Elbow method to find optimal number of clusters for both Information and Description dataset created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile apply_elbow_method.py\n",
    "\n",
    "def apply_elbow(dataset):\n",
    "\n",
    "    wcss = []\n",
    "\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        kmeans = KMeans(n_clusters = i, init = 'k-means++')\n",
    "        kmeans.fit(dataset)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    return wcss\n",
    "\n",
    "\n",
    "# Applying elbow method for information data set\n",
    "score_list_ids = apply_elbow(information_ds_persist['dataset'])\n",
    "\n",
    "\n",
    "# Applying elbow method for description data set\n",
    "score_list_dds = apply_elbow(description_ds_persist['dataset'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot Information dataset Elbow\n",
    "\n",
    "print(\"Plot for IDS: \")\n",
    "\n",
    "plt.plot(range(1,6), score_list_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot description dataset Elbow\n",
    "\n",
    "print(\"Plot for DDS: \")\n",
    "\n",
    "plt.plot(range(1,6), score_list_dds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Comparison among cluster\n",
    "\n",
    "### Cluster the listings in information and description dataset by using the optimal cluster count obtained from the Elbow method\n",
    "\n",
    "### Find similar clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cluster_listings.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile cluster_listings.py\n",
    "\n",
    "# data - is the dataset\n",
    "# k  - number of clusters\n",
    "# @TODO: Need to use Elbow method to decide on\n",
    "# Optimal number of clusters\n",
    "\n",
    "def cluster_documents(data, k):   \n",
    "    \n",
    "    #use k-means to clusterize the songs\n",
    "    kmeans = KMeans(n_clusters=k, init='random') # initialization\n",
    "    kmeans.fit(data) # actual execution\n",
    "    c = kmeans.predict(data)\n",
    "    c_list = list(c)\n",
    "\n",
    "    clustered_list = []\n",
    "\n",
    "    # Creating a multi dimentional array based on k\n",
    "    for c in range(k):\n",
    "        clustered_list.append([])\n",
    "\n",
    "    # Extract the listing ids from indexes\n",
    "    index = 0\n",
    "    for i in c_list:\n",
    "        clustered_list[i].append(index)\n",
    "        index += 1\n",
    "    \n",
    "    return clustered_list\n",
    "\n",
    "# @TODO: Based on the optimial cluster count from Elbow method\n",
    "print('Clustering for Information Dataset: ')\n",
    "ids_c_list = cluster_documents(information_ds_persist['dataset'], 10)\n",
    "\n",
    "print('Clustering for Description Dataset: ')\n",
    "dds_c_list = cluster_documents(description_ds_persist['dataset'], 10)\n",
    "\n",
    "# Jaccard similarity\n",
    "print('Applying Jacard similarity for the clusters: ')\n",
    "cmp_output  = compare_clusters(ids_c_list, dds_c_list)\n",
    "\n",
    "# Getting top 3 similar clusters and use it to generate wordcloud\n",
    "top_3_tuple = sorted(zip(cmp_output['score_list'], cmp_output['comb_list']), reverse=True)[:3]\n",
    "top_3_list = list(top_3_tuple)\n",
    "\n",
    "#Preparing the cluster list for generating word cloud\n",
    "top_c_list = get_similar_clusters(top_3_list, ids_c_list, dds_c_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Word cloud of house descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting create_wordcloud.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile create_wordcloud.py\n",
    "\n",
    "def create_wordcloud(clist, stopwords_flag):\n",
    "\n",
    "    c_index = 0   \n",
    "    \n",
    "    for cluster in clist:\n",
    "        \n",
    "        cur_cluster_words = \" \"\n",
    "        \n",
    "        # Extracting all the words of the listings in current cluster\n",
    "        for list_id in cluster:\n",
    "            cur_cluster_words +=  get_listing_content(list_id, stopwords_flag)\n",
    "        \n",
    "        #strg_cloud = ' '.join(strg_cloud.split())\n",
    "        \n",
    "        wordcloud = WordCloud(width = 300, height = 300, margin = 0, collocations=False).generate(cur_cluster_words)\n",
    "        \n",
    "        plt.imshow(wordcloud, interpolation = \"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.margins(x=0,y=0)\n",
    "        plt.savefig(get_wc_save_path(c_index, stopwords_flag))\n",
    "        #plt.show()\n",
    "\n",
    "        c_index += 1  \n",
    "\n",
    "\n",
    "#Creating wordcloud with top 3 similar clusters\n",
    "# Wordcloud with all the words\n",
    "create_wordcloud(top_c_list, False)\n",
    "\n",
    "# Wordcloud without stopwords\n",
    "create_wordcloud(top_c_list, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of wordcloud:\n",
    "\n",
    "### We applied wordcloud to 2 sets of data, \n",
    "#### 1) without stopwords \n",
    "#### 2) with stopwords(all words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Without stopwords:\n",
    "\n",
    "\n",
    "### Cluster 1\n",
    "\n",
    "![title](wordcloud/cluster_0.png)\n",
    "\n",
    "\n",
    "### Cluster 2\n",
    "\n",
    "![title](wordcloud/cluster_1.png)\n",
    "\n",
    "\n",
    "### Cluster 3\n",
    "\n",
    "![title](wordcloud/cluster_2.png)\n",
    "\n",
    "\n",
    "## With all the words:\n",
    "\n",
    "\n",
    "### Cluster 1\n",
    "\n",
    "![title](wordcloud_all/cluster_0.png)\n",
    "\n",
    "\n",
    "### Cluster 2\n",
    "\n",
    "![title](wordcloud_all/cluster_1.png)\n",
    "\n",
    "\n",
    "### Cluster 3\n",
    "\n",
    "![title](wordcloud_all/cluster_2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
