{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Find the perfect place to stay in Texas!\n",
    "### Group 14 - PavanKumar Alikana, Matteo Cavalletti, Francesca Porcu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The homework consists in analyzing the text of Airbnb property listings and building a search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "import pandas as pd\n",
    "# For displaying search results in a table\n",
    "from IPython.display import HTML, display\n",
    "from os.path import join as pjoin\n",
    "import csv\n",
    "\n",
    "# For persisting indexes in an external file\n",
    "import pickle\n",
    "import math\n",
    "import heapq\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# For word tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# For stop words list\n",
    "from nltk.corpus import stopwords\n",
    "# For word stemming\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#First we import stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#To remove punctuation we use regexptokenizer, but we leave dollar symbol $ because maybe is used in some queries\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$')\n",
    "#we create the stemmer\n",
    "ps = SnowballStemmer('english')\n",
    "\n",
    "# Path to the current working directory to refer to all the files relatively\n",
    "my_path = os.path.dirname(os.path.realpath('__file__'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the main CSV file\n",
    "m = pd.read_csv(\"Airbnb_Texas_Rentals.csv\")\n",
    "doc_len = len(m)\n",
    "\n",
    "# These are commented because we already processed the reviews in to invidual CSV file per review\n",
    "#we found words like '\\\\n' in the dataset so we cleaned it\n",
    "#m = m.replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\\\n',  ' ', regex=True)\n",
    "#m = m.replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\\\t',  ' ', regex=True)\n",
    "#m = m.replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\\\r',  ' ', regex=True)\n",
    "\n",
    "#create tsv files,we droped the first column that was a prroblem\n",
    "#m = m.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines are commented because we already have the individual csv file per review,\n",
    "# after first time we executed this section of code. so we no longer need to execute this every time\n",
    "# Create a separate csv file for every row in the reviews csv file\n",
    "#for i in range(len(m)):\n",
    "#    with open(os.path.join(my_path, 'docu_hw3/doc_' + str(i) + '.tsv', 'w', newline='',encoding='utf-8') as output:\n",
    "#        tsv_output = csv.writer(output, delimiter='\\t')\n",
    "#        tsv_output.writerow(m.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we create the ***vocabulary***. We don't modify directly tsv files because we need them for the output of the search engines, but we create a dictionary that assigns to each file words that would have contained if we had preprocessed them. In particular, we apply to the words contained in each file these procedures:\n",
    "- *Removing stopwords*\n",
    "- *Removing punctuation*\n",
    "- *Stemming*\n",
    "- *Lower-case letters*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_content_persist = {}\n",
    "vocabulary_persist = {}\n",
    "words_persist = {}\n",
    "\n",
    "# Retrieving persisted information for review content and word map\n",
    "# Please create a directory(in your current working directory) with name 'indexes'  \n",
    "content_file = Path(os.path.join(my_path, \"indexes/review_content.pkl\"))\n",
    "vocabulary_file = Path(os.path.join(my_path, \"indexes/vocabulary.pkl\"))\n",
    "words_file = Path(os.path.join(my_path, \"indexes/words.pkl\"))\n",
    "\n",
    "\n",
    "# Retrieving already persisted information\n",
    "\n",
    "# Check if the index file exists, \n",
    "#if yes load the previously persisted indexes and content\n",
    "if content_file.is_file():\n",
    "    with open(content_file, \"rb\") as review_content:\n",
    "        review_content_persist = pickle.load(review_content)\n",
    "        review_content.close()\n",
    "        \n",
    "# Check if the vocabulary file exists, \n",
    "#if yes load the previously persisted vocabulary\n",
    "if vocabulary_file.is_file():\n",
    "    with open(vocabulary_file, \"rb\") as vocabulary:\n",
    "        vocabulary_persist = pickle.load(vocabulary)\n",
    "        vocabulary.close()\n",
    "        \n",
    "# Check if the words file exists, \n",
    "#if yes load the previously persisted words\n",
    "if words_file.is_file():\n",
    "    with open(words_file, \"rb\") as words:\n",
    "        words_persist = pickle.load(words)\n",
    "        words.close()\n",
    "\n",
    "if(len(review_content_persist.keys()) == 0):\n",
    "    \n",
    "    review_word_map = {}\n",
    "    \n",
    "    # We reach here if we don't have indexes already present\n",
    "    print(\"Indexes are being created\")\n",
    "    \n",
    "    #we create the vocabulary of preprocessed documents,but we don't modify the documents because we''l use them in search engine\n",
    "    \n",
    "    for i in range(doc_len):\n",
    "        with open(os.path.join(my_path, 'docu_hw3/doc_' + str(i) + '.tsv'),encoding='utf8') as tsvfile:\n",
    "             tsvreader = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "        \n",
    "        # For review title\n",
    "        l1 = tsvreader[0][4]\n",
    "        \n",
    "        # For review content\n",
    "        l2 = tsvreader[0][7]\n",
    "        \n",
    "        l = l1+ ' ' +l2\n",
    "        l = l.lower()\n",
    "        l = tokenizer.tokenize(l)\n",
    "        \n",
    "        # This array will contain all the valid words in a given review after removing \n",
    "        # all the stop words, punctuations, stemming etc..,, we will use this information\n",
    "        # to find out the term frequency there by tf-idf values\n",
    "        file_words = []\n",
    "        \n",
    "        for r in l :\n",
    "            if not r in stop_words:\n",
    "                sr = ps.stem(r)\n",
    "                \n",
    "                file_words.append(sr)\n",
    "                \n",
    "                if not  sr in review_word_map:\n",
    "                    review_word_map[sr] = [i]\n",
    "                else:\n",
    "                    review_word_map[sr]+=[i]\n",
    "                    \n",
    "                    \n",
    "        review_content_persist[i] = ' '.join(file_words)\n",
    "    \n",
    "    # Saving the content and indexes for the first time\n",
    "    # We made use of pickel python module\n",
    "    #Saving content dictionary\n",
    "    with open(content_file, \"wb\") as review_content:\n",
    "        pickle.dump(review_content_persist, review_content)\n",
    "        review_content.close()\n",
    "    \n",
    "    # Word and Vocabulary indexes based on word map\n",
    "    c = 0\n",
    "    for key in review_word_map:\n",
    "        words_persist[key] = c\n",
    "        vocabulary_persist[c] = review_word_map[key]\n",
    "        c += 1\n",
    "    \n",
    "    #Save vocabulary and words\n",
    "    with open(vocabulary_file, \"wb\") as vocabulary:\n",
    "        pickle.dump(vocabulary_persist, vocabulary)\n",
    "        vocabulary.close()\n",
    "        \n",
    "   \n",
    "    with open(words_file, \"wb\") as words:\n",
    "        pickle.dump(words_persist, words)\n",
    "        words.close()\n",
    "    \n",
    "    \n",
    "                \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search query: roma\n",
      "Cleaned word:  ['roma']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1>No results found. Please try a different query</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word = input('Enter a search query: ')\n",
    "\n",
    "# Cleaning user input similar to what we did for creating indexes for words\n",
    "def clean_input(w):\n",
    "    w_list = []\n",
    "    w = w.lower()\n",
    "    w = tokenizer.tokenize(w)\n",
    "    # Check if we need to do any other preprocessing to improve the efficiency of search results\n",
    "    \n",
    "    for r in w :\n",
    "        if not r in stop_words:\n",
    "            sr = ps.stem(r)\n",
    "            if not  sr in w_list:\n",
    "                w_list.append(sr)\n",
    "    return w_list\n",
    "\n",
    "# Show search results in tabular format\n",
    "def show_results(results, doc_list, isScore):\n",
    "    \n",
    "    if(len(doc_list)):\n",
    "        print('Found ' + str(len(doc_list))  + ' matching reviews to your query')\n",
    "    \n",
    "    if(len(results)):\n",
    "        if(isScore):\n",
    "            tableFormat = '<table border=\"1\"><tr><th>Title</th><th>Description</th><th>City</th><th>URL</th><th>Score</th></tr><tr>{}</tr></table>'\n",
    "        else:\n",
    "            tableFormat = '<table border=\"1\"><tr><th>Title</th><th>Description</th><th>City</th><th>URL</th></tr><tr>{}</tr></table>'\n",
    "        \n",
    "        \n",
    "        display(HTML(tableFormat.format('</tr><tr>'.join('<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in results)\n",
    ")))\n",
    "    else:\n",
    "        display(HTML('<h1>No results found. Please try a different query</h1>'))\n",
    "    \n",
    "\n",
    "\n",
    "word_list = clean_input(word)\n",
    "\n",
    "print(\"Cleaned word: \", word_list)\n",
    "\n",
    "list_doc_list = []\n",
    "\n",
    "for w in word_list:\n",
    "        doc_list = []\n",
    "        \n",
    "        if w in words_persist: \n",
    "            doc_list = vocabulary_persist[words_persist[w]]\n",
    "            \n",
    "        list_doc_list.append(doc_list)  \n",
    "\n",
    "# Initially assinging the list intersection to the matching documents of first word\n",
    "list_intersect = list_doc_list[0]\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return lst3 \n",
    "        \n",
    "for docList in list_doc_list:\n",
    "    list_intersect = intersection(list_intersect, docList)\n",
    "    \n",
    "results = []\n",
    "\n",
    "# Removing the duplicates in the document intersection\n",
    "list_intersect = list(set(list_intersect))\n",
    "\n",
    "i_len = len(list_intersect)\n",
    "\n",
    "if(i_len):\n",
    "    \n",
    "    r_limit = 10\n",
    "    \n",
    "    if(i_len < 10):\n",
    "        r_limit = i_len\n",
    "    \n",
    "    # Showing at most ten results\n",
    "    for doc in list_intersect[:r_limit]:\n",
    "        \n",
    "        # Reading each document based on document id in list intersect \n",
    "        with open(os.path.join(my_path, 'docu_hw3/doc_' + str(doc) + '.tsv'),encoding='utf8') as tsvfile:\n",
    "             tsvreader = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "\n",
    "        title = tsvreader[0][7]        \n",
    "        description = tsvreader[0][4]\n",
    "        city = tsvreader[0][2]\n",
    "        url = tsvreader[0][8]\n",
    "\n",
    "        results.append([title, description, city, url])\n",
    "\n",
    "# Displaying the results\n",
    "show_results(results, list_intersect, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iindex_tf_idf_persist = {}\n",
    "\n",
    "\n",
    "# Check if the index file exists, if yes load the previously persisted indexes and content\n",
    "# Please create a directory(in your current working directory) with name indexes  \n",
    "index_file = Path(os.path.join(my_path, \"indexes/iindex_tf_idf.pkl\"))\n",
    "\n",
    "# Check if the index file exists, if yes load the previously persisted indexes\n",
    "if index_file.is_file():\n",
    "    # Retriving precreated inverted indexes\n",
    "    with open(index_file, \"rb\") as iindex_tf_idf:\n",
    "        iindex_tf_idf_persist = pickle.load(iindex_tf_idf)\n",
    "        iindex_tf_idf.close()\n",
    "        \n",
    "    \n",
    "if(len(iindex_tf_idf_persist.keys()) == 0):\n",
    "    \n",
    "    print(\"Inverted Indexes are being calculated\")\n",
    "\n",
    "    word_iindex = {}\n",
    "\n",
    "    #Creating inverted index using tf-idf and consine similarity\n",
    "    for word in words_persist:\n",
    "        word_doc_list = vocabulary_persist[words_persist[word]]\n",
    "        word_iindex[word] = []\n",
    "\n",
    "        # Store indexes based on number of times a particular word is present in a given document\n",
    "        for doc in word_doc_list:\n",
    "            doc_content = review_content_persist[doc]\n",
    "            # Pushing the term frequency with document id\n",
    "            word_iindex[word].append([doc, doc_content.split().count(word)])\n",
    "\n",
    "    # Store indexes based on tf-idf\n",
    "    docs_length = len(review_content_persist.keys())\n",
    "    iindex_tf_idf_persist = word_iindex\n",
    "\n",
    "    for key, word in iindex_tf_idf_persist.items():\n",
    "        # find out the relative importance of a particular terms relating it to document count\n",
    "        idf= math.log10( docs_length / len(word) )\n",
    "\n",
    "        for elem in word:\n",
    "            # Add the document score corresponding to a particular term which we then use in the \n",
    "            # search results ranking of documents\n",
    "            elem[1] = idf * elem[1]\n",
    "    \n",
    "    # Persisting the indexes calculated \n",
    "    with open(index_file, \"wb\") as iindex_tf_idf:\n",
    "        pickle.dump(iindex_tf_idf_persist, iindex_tf_idf)\n",
    "        iindex_tf_idf.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_qcos = {}\n",
    "dict_norm = {}\n",
    "\n",
    "#print(\"Search started\")\n",
    "\n",
    "# Need to remove this 100 hardcoding\n",
    "for doc in list_intersect[:100]:\n",
    "    \n",
    "    num = 0\n",
    "    \n",
    "    #print(\"Current document ID: \" + str(doc))\n",
    "    \n",
    "    # Calculating numerator of the cosine similarity equation\n",
    "    \n",
    "    for word in word_list:\n",
    "        w_index = iindex_tf_idf_persist[word]\n",
    "        w_i_len = len(w_index)\n",
    "        for i in range(w_i_len):\n",
    "            if w_index[i][0] == doc: \n",
    "                num +=  w_index[i][1]\n",
    "                \n",
    "    dict_qcos[doc]=num\n",
    "\n",
    "    \n",
    "    # Calculating denominator of the cosine similarity equation\n",
    "    norm = 0\n",
    "    for word in iindex_tf_idf_persist.values():\n",
    "        for i in range(len(word)):\n",
    "            if word[i][0] == doc:\n",
    "                norm +=  word[i][1]**2\n",
    "    \n",
    "    dict_norm[doc]=math.sqrt(norm)\n",
    "\n",
    "#print(\"Numerator and Denominator calculated\")\n",
    "\n",
    "# Once numerator and denominator is calculated find the score of each document in the intersection list \n",
    "# By applying the consine similarity formala\n",
    "for doc,num in dict_qcos.items():\n",
    "    # Eleminating divided by zero problem to check if the normalization value for a document is non-zero\n",
    "    if dict_norm[doc] != 0:\n",
    "        dict_qcos[doc] = num/(math.sqrt(len(word_list))*dict_norm[doc])\n",
    "\n",
    "#print(\"Cosine similarity done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>No results found. Please try a different query</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Applying heap data structure to print to top-k documents\n",
    "\n",
    "h = []\n",
    "results = []\n",
    "\n",
    "for doc in dict_qcos.keys():\n",
    "    \n",
    "    # Reading the document meta data to print in the search results\n",
    "    \n",
    "    with open(os.path.join(my_path, 'docu_hw3/doc_' + str(doc) + '.tsv'),encoding='utf8') as tsvfile:\n",
    "         tsvreader = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "    \n",
    "    title = tsvreader[0][7]        \n",
    "    description = tsvreader[0][4]\n",
    "    city = tsvreader[0][2]\n",
    "    url = tsvreader[0][8]\n",
    "    \n",
    "    # Pushing the document information to heap data structure\n",
    "    \n",
    "    heapq.heappush(h,(dict_qcos[doc], title, description, city, url))\n",
    "\n",
    "#print(\"Applying Heap\")    \n",
    "# Applying max heap algorithm\n",
    "heapq._heapify_max(h)\n",
    "\n",
    "limit = 10\n",
    "doc_len = len(dict_qcos.keys())\n",
    "\n",
    "if(doc_len < 10):\n",
    "    limit = doc_len\n",
    "\n",
    "for i in range(limit):\n",
    "    # Popping the document with maximum score at every step and adding it to the result list\n",
    "    \n",
    "    # Since the data structure used in heap is a tuple, we convert it in to list for ease of manipulation\n",
    "    results.append(list(heapq.heappop(h)))\n",
    "    \n",
    "    # re-applying the max heap algorithm\n",
    "    heapq._heapify_max(h)\n",
    "\n",
    "#print(\"Applying Heap\")  \n",
    "\n",
    "#print(results)\n",
    "results_formatted = []\n",
    "\n",
    "for i in results:\n",
    "    \n",
    "    first = round(i.pop(0), 4)\n",
    "    i.append(first) \n",
    "    #first = round(float(results[i].pop(0)), 4)\n",
    "    #results[i] = results[i].append(first)\n",
    "    results_formatted.append(i)\n",
    "\n",
    "\n",
    "# Displaying the results\n",
    "show_results(results_formatted, [], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New score is:\n",
    "- *average price per night (av_r_n_u)*\n",
    "- *n° of bedrooms (bedr_c_u)*\n",
    "- *zone (zone_u)*\n",
    "\n",
    "**Average price per night**: we simply use average price per night from the tsv files and from this variable any house can get at maximum 2 points and at least 0. Maximum number of points (2) is achieved if the price is lower than half of the price suggested by the user, instead if price is two times the requested price the house get 0. To get 1 point you need to have the same price or lower than 1.25 times the suggested price. Naturally, other intermediate marks can be achieved.\n",
    "\n",
    "**n° of bedrooms**: if the n° of bedrooms requested are more or the same, house gets 1 point; if there are less bedrooms, house gets a lower mark. If there are three bedrooms less than requested or worse, house gets 0 point. \n",
    "\n",
    "**zone**: We found a set of coordinates (31,169621, -99,683617) that can be considered the center of texas; that is near to the city of Brady (County of McCulloch), so we divided Texas in four zones according to this center (like a cartesian coordinate system). Zones are: North East (NE), North West (NW), South East (SE) and South West (SW). If the house is in the requested zone it gets 1 point else 0. Is important to clarify that we didn't use cities because, given the fact that there are cities with the same name in different counties (e.g. we have 3 Austin), we can't use them because they can be confused.\n",
    "\n",
    "Finally we can calculate the Score as sum/4 where sum is sum of the accomulated scores from individul fields based on user choices and 4 is maximum number points that can be accumulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please, enter maximum price that you can pay(e.g. 10): 500\n",
      "please, enter the number of bedrooms that you need(e.g 5): 2\n",
      "please enter the zone you want to reside(e.g. NE): ariosto\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1>No results found. Please try a different query</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "av_r_n_u = 0\n",
    "bedr_c_u = 1000\n",
    "zone_u = 'NA'\n",
    "\n",
    "try:\n",
    "    av_r_n_u = int(input('please, enter maximum price that you can pay(e.g. 10): '))\n",
    "except:\n",
    "    print('No price suggested')\n",
    "    pass\n",
    "try:\n",
    "    bedr_c_u = int(input('please, enter the number of bedrooms that you need(e.g 5): '))\n",
    "except:\n",
    "    print('No indication about the needed number of bedrooms')\n",
    "    pass\n",
    "try:\n",
    "    zone_u = input('please enter the zone you want to reside(e.g. NE): ')\n",
    "except:\n",
    "    print('No zone was indicated')\n",
    "    pass\n",
    "list_ord =[]\n",
    "\n",
    "n_h = []\n",
    "\n",
    "for doc in list_intersect:\n",
    "    with open(r'docu_hw3\\doc_'+ str(doc) + '.tsv',encoding='utf8') as tsvfile:\n",
    "         tsvreader = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "    title = tsvreader[0][7]        \n",
    "    description = tsvreader[0][4]\n",
    "    latitude = tsvreader[0][5]\n",
    "    longitude = tsvreader[0][6]\n",
    "    url = tsvreader[0][8]\n",
    "    price = list(tsvreader[0][0])\n",
    "    eff_price = ''\n",
    "    for i in range(1,len(price)):\n",
    "        eff_price += price[i]\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        av_r_n = int(eff_price)\n",
    "        bedr_c = int(tsvreader[0][1])\n",
    "    except ValueError:\n",
    "        av_r_n = 1000000\n",
    "        bedr_c = 1\n",
    "    \n",
    "    sum_values = 0\n",
    "    if av_r_n >= 2*av_r_n_u :\n",
    "               sum_values += 0          \n",
    "    elif av_r_n < 2*av_r_n_u and av_r_n >= 1.75*av_r_n_u:\n",
    "               sum_values += 0.25\n",
    "    elif  av_r_n < 1.75*av_r_n_u and av_r_n >= 1.5*av_r_n_u:\n",
    "               sum_values += 0.50  \n",
    "    elif av_r_n < 1.5*av_r_n_u and av_r_n >= 1.25*av_r_n_u:\n",
    "               sum_values += 0.75 \n",
    "    elif av_r_n < 1.25*av_r_n_u and av_r_n >= av_r_n_u:\n",
    "               sum_values += 1           \n",
    "    elif av_r_n < av_r_n_u and av_r_n >=0.75*av_r_n_u:\n",
    "               sum_values += 1.25\n",
    "    elif av_r_n < 0.75*av_r_n_u and av_r_n >=0.5*av_r_n_u :\n",
    "               sum_values += 1.5\n",
    "    elif av_r_n < 0.5*av_r_n_u :\n",
    "               sum_values += 2.\n",
    "    \n",
    "    \n",
    "    if bedr_c >= bedr_c_u:\n",
    "               sum_values += 1\n",
    "    elif bedr_c == bedr_c_u-1:\n",
    "               sum_values += 0.75\n",
    "    elif bedr_c == bedr_c_u-2:\n",
    "               sum_values += 0.5\n",
    "    elif bedr_c <= bedr_c_u-3:\n",
    "               sum_values += 0 #only to make index easier to understand , no practical effect\n",
    "            \n",
    "    try:\n",
    "        la = float(latitude)\n",
    "        lo = float(longitude)\n",
    "        if la <=31.169621 and lo <= -99.683617:\n",
    "             zone ='SW'\n",
    "        elif la <=31.169621 and lo > -99.683617:\n",
    "             zone ='SE'\n",
    "        elif la >31.169621 and lo > -99.683617:\n",
    "             zone ='NE'\n",
    "        elif la >31.169621 and lo <= -99.683617:\n",
    "             zone ='NW'\n",
    "        if zone == zone_u:\n",
    "            sum_values +=1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    score = round(sum_values/4,2)\n",
    "    \n",
    "    #list_ord.append([title, description, city, url, str(score),score])\n",
    "    \n",
    "    heapq.heappush(n_h,(score, title, description, city, url))\n",
    "\n",
    "results = []\n",
    "\n",
    "# Applying heapify max algorithm\n",
    "heapq._heapify_max(n_h)\n",
    "\n",
    "limit = 10\n",
    "d_len = len(list_intersect)\n",
    "\n",
    "if(d_len < 10):\n",
    "    limit = d_len\n",
    "\n",
    "for i in range(limit):\n",
    "    results.append(list(heapq.heappop(n_h)))\n",
    "    heapq._heapify_max(n_h)\n",
    "\n",
    "#print(\"Applying Heap\")  \n",
    "\n",
    "#print(results)\n",
    "results_formatted = []\n",
    "\n",
    "for i in results:\n",
    "    first = round(i.pop(0), 4)\n",
    "    i.append(first) \n",
    "    results_formatted.append(i)\n",
    "    \n",
    "\n",
    "# Displaying the results\n",
    "show_results(results_formatted, [], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Step: Make a nice visualization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important feature of Airbnb is the search on the map. \n",
    "\n",
    "Our tool will works in the following way:\n",
    "\n",
    "- Takes in input a set of coordinates and a maximum distance from the coordinates.\n",
    "- Generate a map, with a circle of the given radius, where the center is represented by the coordinates given in input.\n",
    "- Shows the houses that are inside the circle of the given radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We drop rows where latitude and/or longitude column contains missing values (NaN)\n",
    "m = m.dropna(subset=['latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a latitude: 5\n",
      "Enter a longitude: 10\n",
      "Enter distance range (in km): 10\n"
     ]
    }
   ],
   "source": [
    "#We ask as input latitude, longitude and a maximum distance to generate the radius\n",
    "lat = float(input('Enter a latitude: '))\n",
    "lon = float(input('Enter a longitude: '))\n",
    "dis = float(input('Enter distance range (in km): '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopy\n",
    "import folium\n",
    "from geopy import distance\n",
    "\n",
    "#We create the map with given coordinates\n",
    "mp = folium.Map(location = [lat, lon], zoom_start = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search represents the set of given coordinates\n",
    "search = (lat, lon)\n",
    "\n",
    "#Now, we create the marker associated with input set of coordinates, that we call origin\n",
    "folium.Marker(location = [lat, lon], popup = 'origin', icon = folium.Icon(color = 'green', icon = 'home')).add_to(mp)\n",
    "\n",
    "#Then, we generate the circle with input set of coordinates as center. \n",
    "folium.Circle(location = [lat, lon], radius = dis * 1000).add_to(mp)\n",
    "\n",
    "#For each house that is in the given distance from the input set of coordinates, we create a marker with price that \n",
    "#can be clicked to access the house web page\n",
    "for row in m.itertuples():\n",
    "    if distance.distance(search, (row.latitude, row.longitude)).km <= dis:\n",
    "        folium.Marker(location = [row.latitude, row.longitude], popup = folium.Popup('<a href=' + row.url + '>' + row.average_rate_per_night + ' </a>')).add_to(mp)\n",
    "        \n",
    "mp.save('map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
